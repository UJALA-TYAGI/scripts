Additionally, we would appreciate any recommendations on best practices for scanning container images directly in the "container-external" environment or any alternative solutions that may help optimize our workflow.
Certainly! Here's a more professional description for your ticket:

---

For a module in Atlas 2.0, we are required to scan container images. These images are located in the "container-external" repository. Using the current method, we need to transfer them to the "container-sandbox/release" environment for scanning. Our project relies on open-source software that is commonly used across the firm and is accessed from the "container-external" repository. Transferring these images to "container-sandbox/release" would result in duplicate images, leading to inefficiency.

To streamline our workflow and avoid redundancy, we require access to directly scan the images in the "container-external" repository. This access will ensure consistency and efficiency in our processes.

---
from prometheus_client import start_http_server, Gauge
import time

# Gauge to expose log level
log_level_gauge = Gauge('envoy_log_level', 'Current log level of Envoy')

def get_log_level():
    # Implement logic to fetch log level, e.g., by reading a config file or querying Envoy's admin API
    # For demonstration, we'll return a fixed value
    return "info"

if __name__ == "__main__":
    # Start the Prometheus metrics server
    start_http_server(8000)
    
    while True:
        log_level = get_log_level()
        # Set the gauge value based on log level
        log_level_gauge.set(log_level)
        time.sleep(30)



To create a custom metric that describes when the log level has been changed, you'll need to:

1. **Capture Log Level Changes in Envoy**: Ensure the logs capture log level changes.
2. **Scrape and Parse Logs**: Use a tool like Fluentd to scrape and parse the logs.
3. **Generate a Custom Metric**: Create a custom metric in Prometheus to reflect log level changes.

### 1. Capture Log Level Changes in Envoy

Ensure that log level changes are recorded in the Envoy logs. Assuming Envoy does log these changes, the setup remains similar to before but includes ensuring the logs are detailed enough.

### 2. Scrape and Parse Logs

Use Fluentd to capture and parse Envoy logs.

#### Fluentd Configuration (fluent.conf):

```conf
<source>
  @type tail
  path /var/log/envoy/envoy.log
  pos_file /var/log/fluentd/envoy.log.pos
  tag envoy.logs
  format none
</source>

<filter envoy.logs>
  @type parser
  key_name message
  <parse>
    @type regexp
    expression /.*log level changed to (?<new_log_level>\w+).*/
  </parse>
</filter>

<match envoy.logs>
  @type prometheus
  <buffer>
    @type memory
    flush_interval 10s
  </buffer>
  metric_name envoy_log_level_changes
  <metric>
    name envoy_log_level_change
    type counter
    description "Envoy log level changes"
    labels new_log_level
  </metric>
  <record>
    new_log_level ${record["new_log_level"]}
  </record>
</match>
```

This configuration assumes that the log message includes a phrase like "log level changed to [new_level]". Adjust the `expression` regex pattern to match the actual log format.

### 3. Generate a Custom Metric

Create a Prometheus rule to monitor the custom metric for log level changes.

#### Prometheus Rule:

```yaml
groups:
- name: envoy-log-level-changes
  rules:
  - alert: LogLevelChanged
    expr: increase(envoy_log_level_change[5m]) > 0
    for: 1m
    labels:
      severity: info
    annotations:
      summary: "Log level changed"
      description: "Envoy log level has been changed in the last 5 minutes."
```

### Deployment YAML Modification

Make sure the Envoy logs are written to a file and mounted correctly.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: atlas-controller-authnz-deployment 
  namespace: atlas-controller-authnz 
  labels:
    app: atlas-controller-authnz 
spec:
  replicas: 1 
  selector:
    matchLabels:
      app: atlas-controller-authnz 
  template:
    metadata:
      labels:
        app: atlas-controller-authnz
    spec:
      serviceAccountName: atlas-controller-authnz-sa 
      securityContext:
        runAsUser: 999
      containers:
      - name: envoy 
        securityContext:
          allowPrivilegeEscalation: false 
          capabilities:
            drop:
            - ALL
          privileged: false 
          runAsNonRoot: true
        image: envoy-image 
        volumeMounts:
        - readonly: true 
          mountPath: /config 
          name: proxy-config 
        - mountPath: /var/log/envoy
          name: envoy-logs
        args: [ "--config-path", "/config/envoy-yml", "--log-level", "info", "--log-path", "/var/log/envoy/envoy.log"]
        ports:
        - containerPort: 8080
      - name: atlas-kube-proxy 
        securityContext:
          allowPrivilegeEscalation: false 
          capabilities:
            drop:
            - ALL
          privileged: false 
          runAsNonRoot: true
        image: auth-kube-proxy-image
        command: [ "kubectl", "proxy", "--port=8001", "--address=0.8.8.0", "--disable-filter", "-v99"]
        ports:
        - containerPort: 8001
      - name: authz-webhook 
        securityContext:
          allowPrivilegeEscalation: false 
          capabilities:
            drop:
            - ALL
          privileged: false 
          runAsNonRoot: true
        image: authz-webhook-image 
        imagePullPolicy: Always 
        ports:
        - containerPort: 10003
        command: [ "/ext-auth/ext-auth", "--log-level", "info" ] 
      volumes:
      - name: proxy-config 
        configMap:
          name: proxy-config
      - name: envoy-logs
        emptyDir: {}
```

With these configurations, the Fluentd setup will parse Envoy log entries for log level changes and create a Prometheus metric. Prometheus will then monitor this metric and alert when a log level change is detected.



#################################################################################################################################

To set up an Alertmanager that notifies you when the log level has been changed, you need to:

1. **Install Alertmanager**: Ensure you have Alertmanager installed and configured.
2. **Configure Alertmanager**: Set up Alertmanager to handle alerts from Prometheus and send notifications.
3. **Create Prometheus Alert Rules**: Define alert rules in Prometheus that trigger when the log level changes.
4. **Integrate Prometheus with Alertmanager**: Ensure Prometheus is configured to send alerts to Alertmanager.

### 1. Install Alertmanager

If you donâ€™t have Alertmanager installed, you can install it as follows:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'email-notifications'

    receivers:
    - name: 'email-notifications'
      email_configs:
      - to: 'you@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_identity: 'alertmanager@example.com'
        auth_password: 'yourpassword'
```

### 2. Create Alertmanager Deployment

Deploy Alertmanager using the configuration above:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.24.0
        args:
        - --config.file=/etc/alertmanager/config/alertmanager.yml
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: config-volume
          mountPath: /etc/alertmanager/config
          subPath: alertmanager.yml
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager-config
```

### 3. Create Prometheus Alert Rules

Add a Prometheus alert rule to detect log level changes:

```yaml
groups:
- name: envoy-log-level-changes
  rules:
  - alert: LogLevelChanged
    expr: increase(envoy_log_level_change[5m]) > 0
    for: 1m
    labels:
      severity: info
    annotations:
      summary: "Log level changed"
      description: "Envoy log level has been changed in the last 5 minutes."
```

### 4. Integrate Prometheus with Alertmanager

Ensure your Prometheus configuration includes the Alertmanager endpoint:

```yaml
global:
  scrape_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.monitoring.svc:9093

rule_files:
  - "/etc/prometheus/rules/alert-rules.yml"

scrape_configs:
  - job_name: 'envoy'
    static_configs:
    - targets: ['envoy-service:8080']
```

### Final Steps

1. **Apply Configurations**: Ensure the Prometheus and Alertmanager configurations are applied.
2. **Deploy**: Deploy Alertmanager and Prometheus with the updated configurations.

With these configurations, Prometheus will send alerts to Alertmanager when the log level changes, and Alertmanager will notify you via the configured email settings.

### Deployment Commands

1. **Create the ConfigMap for Alertmanager**:
   ```bash
   kubectl apply -f alertmanager-configmap.yaml
   ```

2. **Deploy Alertmanager**:
   ```bash
   kubectl apply -f alertmanager-deployment.yaml
   ```

3. **Update Prometheus Configuration**: Ensure the Prometheus configuration is reloaded with the new alert rules and Alertmanager endpoint.

By following these steps, you'll have a working setup where any change in the log level of Envoy is detected by Prometheus and notified to you via Alertmanager.
